# Questions

## 1. What's loss function and cost functions ? are they same ?
   * [[src1]](https://youtu.be/SHEPb1JHw5o?t=441)
 
## 2. What loss function give us ? What cost function give us ? What is the use of this functions ?
 
## 3. If we have neural network with tow layers, first have 3 neu. second 1 neu, and we apply a linear function for first layer and sigmoid func. for second layer, we end up with a simple logistic regression algorithm, in another word the neural network change into logistic regression.

   * [[src1]](https://youtu.be/NkOv_k7r6no?t=188)

## 4. What's gradient ? Why we use gradient ? What gradient tell us ?

**Gradient** of any function `[src2]` is a vector with its partial derivatives.

**Gradient** `[src3]` is talk about change. this slope telling us how quickly is change, by finding the gradient of the that graph bellow we know how quickly is changing.

<img src="https://raw.githubusercontent.com/SalAlba/machine-learning/master/topics/images/1.png" >


**Gradient** `[src4]` is another word for "slope". The higher the gradient of a graph at a point, the steeper the line is at that point.

+ A positive gradient means that the line slopes goind upwards.
+ A negative gradient means that the line slopes downwards.


**Reosurces**

* **COME BACK** [[src1] Math > Multivariable calculus > Derivatives of multivariable functions > Gradient and directional derivatives > Gradient](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient)

* [[src2]](https://youtu.be/tIpKfDc295M?t=166)

* [[src3]](https://www.youtube.com/watch?v=6zgBUZuC-p8)

* [[src4]](https://revisionmaths.com/gcse-maths-revision/algebra/gradients-and-graphs)

## 5. What's Gradient Descent ?

+ Gradient descent measures the local gradient of the error function with regards to the parameter vector Î¸.

+ When gradient is zero you are in the bottom (the minimum), keep in minde could be many minimums local and global.

+ An important parameter in Gradient Descent is the size of the steps, determined by
the learning rate hyperparameter.

**Resources**

* [IBM]  https://www.ibm.com/cloud/learn/gradient-descent
* [[src1] Hands on ML with tens. and sklearn](#)
* [[src2]](https://medium.com/@rrfd/what-is-a-cost-function-gradient-descent-examples-with-python-16273460d634)

## 6. What's vector ? Why we want vector ?

## 7. What's normal distribution ? Why, How, Where, What ?


## 8. Iterators vs. Generators in Python

**Resources**
* [[src1]](https://www.youtube.com/watch?v=u0JnTdAcGns)


## 9. How to build NLP model that generalizes? and how to train/validat/test ?


**Resources**
* [[src1]](kdnuggets.com/2019/01/solve-90-nlp-problems-step-by-step-guide.html)
